<h2>[深度学习笔记(DeepLearning)] :memo: </h2> 
 
> 吴恩达 deeplearning.ai 视频笔记，博士师兄带头整理，赠人玫瑰，手留余香。

```
目录
第一门课 神经网络和深度学习(Neural Networks and Deep Learning) 
一、 第一周：深度学习引言(Introduction to Deep Learning)
1.1 欢迎(Welcome)
1.2 什么是神经网络？ (What is a Neural Network)
1.3 神经网络的监督学习(Supervised Learning with Neural Networks)
1.4 为什么深度学习会兴起？ (Why is Deep Learning taking off?) 
1.5 关于这门课(About this Course) 
1.6 课程资源(Course Resources)

第二周：神经网络的编程基础(Basics of Neural Network programming) 
2.1 二分类(Binary Classification)
2.2 逻辑回归(Logistic Regression)
2.3 逻辑回归的代价函数(Logistic Regression Cost Function)
2.4 梯度下降法(Gradient Descent)
2.5 导数(Derivatives)
2.6 更多的导数例子(More Derivative Examples)
2.7 计算图(Computation Graph)
2.8 计算图的导数计算（Derivatives with a Computation Graph）
2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent） 
2.10 m 个样本的梯度下降(Gradient Descent on m Examples)
2.11 向量化(Vectorization)
2.12 向量化的更多例子（More Examples of Vectorization）
2.13 向量化逻辑回归(Vectorizing Logistic Regression) 
2.14 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression's Gradient）
2.15 Python 中的广播（Broadcasting in Python）
2.16 关于 python _ numpy 向量的说明（A note on python or numpy vectors）
2.17 Jupyter/iPython Notebooks 快速入门（Quick tour of Jupyter/iPython Notebooks）
2.18（选修）logistic 损失函数的解释（Explanation of logistic regression cost function）

第三周：浅层神经网络(Shallow neural networks)
3.1 神经网络概述（Neural Network Overview）
3.2 神经网络的表示（Neural Network Representation）
3.3 计算一个神经网络的输出（Computing a Neural Network's output）
3.4 多样本向量化（Vectorizing across multiple examples） 
3.5 向量化实现的解释（Justification for vectorized implementation
3.6 激活函数（Activation functions）
3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）
3.8 激活函数的导数（Derivatives of activation functions）
3.9 神经网络的梯度下降（Gradient descent for neural networks）
3.10（选修）直观理解反向传播（Backpropagation intuition）
```
