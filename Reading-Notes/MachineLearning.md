<h2>《机器学习》 :books: </h2> 

> 周志华 著  清华大学出版社 

```html
1.机器学习的本质任务是预测。
  “机器学习”也是一门学科，研究怎样使得计算机更好地学习，亦即，是一门研究“学习算法”的学科，主要任务是评估“学习算法”的好坏
以及开发新的“学习算法”。这里的“学习算法”是计算机的学习方法，本质上是一种基于现有的数据产生预测模型的算法。

2.“假设空间”里的“假设”指的是假设函数，也就是机器学习的成果。例如我们做分类学习，那么通过数据训练后得到的分类模型
就是我们得到的假设.假设空间是指所有可能假设组成的空间。也可以说是所有在表达形式上符合任务要求的假设函数的集合。
  对于西瓜分类任务，我们要获得的假设函数的形式是：好瓜 → (色泽=*) ^ (根蒂=*) ^ (敲声=*).
  假设“色泽”、“根蒂”、“敲声”3个特征都有3种可能取值，那就有 4 * 4 * 4 + 1 = 65 种可能假设，亦即假设空间的大小为 65。

3.“归纳偏好”：在西瓜分类问题中，可能由于数据集的原因，我们会得到多个符合数据集的假设函数，比如：
    好瓜 → (色泽 = 墨绿) ^ (根蒂 = 蜷缩) ^ (敲声 = 沉闷)
    好瓜 → (色泽 = 青绿) ^ (根蒂 = *) ^ (敲声 = 沉闷)
这所有训练后得到的假设组成的空间称为“版本空间”。
   那么版本空间中哪一个假设比较好？
   如果我们认为越精细越好，则选择: 好瓜 → (色泽 = 墨绿) ^ (根蒂 = 蜷缩) ^ (敲声 = 沉闷)
   如果我们认为越粗略越好，则选择: 好瓜 → (色泽 = 青绿) ^ (根蒂 = *) ^ (敲声 = 沉闷)
   像上面那样，计算机的学习算法基于某种偏好认为某个假设比其他假设好，那么我们说这个学习算法有“归纳偏好”。
事实上所有“学习算法”都有归纳偏好，而且一般来说会偏好那些形式简单的假设。

4.人类观察事物时，是通过观察事物的本质特征来认识事物的。比如观察西瓜，会观察西瓜的色泽、根蒂、敲声等特征。
  假设我们收集了一批关于西瓜的数据：
   (色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)
   (色泽 = 墨绿; 根蒂 = 稍蜷; 敲声 = 沉闷)
   (色泽 = 浅白; 根蒂 = 硬挺; 敲声 = 清脆)
    ······
   假设我们希望用这一批数据来让计算机学习:
  (1) 样本、示例、记录 —— 这批数据里的每对括号。
  (2) 数据集 —— 这组样本(示例、记录)的集合。
  (3) 特征、属性 —— 色泽、根蒂、敲声等反映一个事物的本质的可观察方面。
  (4) 属性值 —— 青绿、墨绿、蜷缩、浊响等，是属性的取值。
  (5) 属性空间、样本空间、输入空间 —— 属性长成的空间。这似乎是线性代数的语言，亦即把属性当作坐标轴，形成一个空间，
那么样本就是这个空间中一个个的点。例如：把“色泽”、“根蒂”、“敲声”作为坐标轴，则生成了一个三维空间，
每个西瓜都是这个空间里的一个点。
  (6) 维数 —— 样本空间的坐标轴数，也就是数据集的特征数量。本例中的维数是 3。
  (7) 假设 —— 也称假设函数，指计算机通过学习后得到的一个函数(预测模型)。
  (8) 标记 —— 关于样本结果的信息，比如一个(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)的西瓜是好瓜，
那么“好瓜”就是(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)这个样本的标记。
  (9) 样例 —— 带有标记的样本，比如(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)，好瓜。
  (10) 标记空间、输出空间 —— 所有标记的集合。本例中就是指{好瓜、坏瓜}。
  (11) 泛化 —— 如果用某个数据集的样本训练出的一个模型(假设函数)，能够适用于新的样本数据，就说这个模型具有泛化能力。
模型能适用于越多的新数据，则说明其泛化能力越强。

5.NFL(No Free Lunch)定理，翻译过来就是“没有免费午餐”定理，说的是在机器学习中，在没有给定具体问题的情况下，
或者说面对的是所有问题的情况下，没有一种算法能说得上比另一种算法好。换成我们的俗话讲，就是“不存在放之四海而皆准的方法”。
只有在给定某一问题，比如说给“用特定的数据集给西瓜进行分类”，才能分析并指出某一算法比另一算法好。
这就要求我们具体问题具体分析，而不能指望找到某个算法后，就一直指望着这个“万能”的算法。
  这大概也是 no free lunch 名字的由来吧。
  定理推导的思路是证明对于某个算法 a，它在训练集以外的所有样本的误差，与 a 本身无关。
  误差是怎样表示，或者说怎样计算出来的？简单起见，只考虑二分类问题。那么误差就是分类器错判的个数与样本总数的比
      E = 误判数 / 总数。
  其次我们要明确，一个算法，会产生很多不同的假设。更详细的说，一个算法的结果就是一个函数 h，但是 h 的参数不同，
那么就会有 h1，h2 等不同的假设函数。最典型的是 h = kx + b。只要参数 k、b 不同，那么函数 h 就不同了。
  对于某个算法 a，它在训练集以外的所有样本的误差，就是它所能产生的所有假设 h，在训练集以外的所有样本上的误判率的和。
  对于某个假设 h，“h 在某个数据集上的误差”与“在某个数据集中抽取一个能让 h 误判的样本的概率”是等价的问题。
```

> NFL(No Free Lunch)定理的具体证明详见：<a href="https://www.jianshu.com/p/cbe8e0fe7b2c">url</a>

```html
6.P5 假设空间的规模问题。
  (1) 某一属性值无论取什么都合适，我们用通配符 “*” 来表示。
  (2) 世界上没有，我们用 “∅” 来表示。

7.“奥卡姆剃刀”原则：“若有多个假设与观察一致，则选最简单的那个”。
   预测((离散值：分类-二分类(正类, 反类/负类), 多分类); 连续值)。
   学习任务(监督：有标记(分类, 回归); 无监督：无标记(聚类))。
   科学推理(归纳(特殊->一般：泛化); 演绎(一般->特殊：特化))。
   归纳学习(广义(从样例中学习); 狭义(从训练数据中学得概念，最基本：布尔概念学习))。

8.评估学习器的基本思想是：
  (1) 学习器误差越小越好。对于分类任务，分类错误的样本数占总样本数的比率越小越好。
对于回归预测，预测值与真实值的差越小越好。
  (2) 学习器泛化能力越强越好。也就是说学习能力不仅在训练样本上要表现好，在新的样本上的表现也要好。
不能像书呆子一样在学校表现很好，但一进入社会就一塌糊涂。因此，我们通常衡量一个学习器的泛化误差，
也就是一个训练好的学习器在新样本上的误差表现。

9.当只有一个数据集，并且既要训练，又要测试的时候，怎么办？
  (1) 留出法：当只有一个数据集的时候，用一部分来训练，一部分来测试。而且训练数据和测试数据没有交集。
通常会用 60% 到 80% 的数据作为训练集，剩下的作为测试集。需要注意的是，在选择训练集(或者测试集)的时候要采用分层抽样的方法。
就像刷题一样，训练集和测试集都要有相近比例的题型，不能训练集全是选择题，测试集全是论述题，
应该训练集和测试集都包含选择题和测试题，而且比例要一致，都是八成选择题，两成论述题。
一次的训练-测试结果可能不够科学，最好划分不同的训练集和测试集，做多次训练-测试，将测试结果(错误率、查准率之类的)取平均。
  (2) 交叉检验法：这是在 “留出法” 的基础上改进的方法。先将数据集分为 k 个大小相似的互斥子集
(当然，每个子集的产生都要用分层抽样进行)。每次用 k-1 个子集作为训练集，剩下的一个作为测试集。
这样就可以进行 k 次训练-测试。k 的测试结果的平均值就是最终的测试结果。
  (3) 自助法：上述两种方法都是在原本作为训练集的数据中抽出一部分作为测试集，因此训练集的规模不可避免地减少了，
训练效果也就受到了影响。自助法则是一中比较好的缓解方法。假设有一个包含 m 个样本的数据集 D。
对这个数据集进行 m 次有放回的抽样，则得到了一个含有 m 个样本的数据集 D'。 D' 相对于原数据集 D，规模没有减少，
只是 D' 中有部分样本是重复出现的。所以在抽样中没有抽到的样本就作为测试集，D' 就作为训练集。按照概率论推导可知，
一般来说抽样中会有三分之一的样本没有被抽到，也就是说测试集大小为数据集 D 大小的三分之一。
   在数据集比较大时多采用留出法和交叉检验法，当数据集比较小是采用自助法。

10.在测试一个学习器时，有哪些测试指标可以使用？
  (1) 错误率(error)：最常用的测试指标就是错误率(或者精度)。对于一次分类任务，如果分类错误的样本数为 a，总样本数为 m，
则错误率 E = a / m.(精度为 1 - a / m)。比如为 100 个西瓜分类，有 10 个分错了，错误率就是 10%。
  (2) 均方误差(mean squared error)：“错误率”一般针对分类任务，回归预测则用均方误差，即各次预测值与真实值的差的平方的和：
$$ \sum_{i = 1}^n (y_预 - y_真)^2 $$; 可以认为是各次预测的误差的累加。
  (3) 查准率(precision)：也就是检索出来的结果中准确的结果所占的比例。比如找出 100 个西瓜中的好瓜，找出 50 个，
但这 50 个中只有 40 个是真正的好瓜，则查准率为 80%。
  (4) 查全率(recall)：也就是希望检索的结果中被检索出来的比例。比如找出 100 个西瓜中的好瓜，找出 40 个，
但真正的好瓜有 50个，则查全率为 80%。
  (5) ROC 曲线: 很多二分类学习器的分类方法是计算出每一个样本作为正例的概率，然后按照概率大小对样本排序，
最后确定一个临界概率(阀值)，大于临界概率的认定为正例，其余为反例。以西瓜分类为例。有些西瓜是好瓜的概率高，
有些西瓜是好瓜的概率低。把这些西瓜按照概率排序，然后取 50% 作为临界概率。概率大于 50% 的认为是好瓜，否则为坏瓜。
因此这个排序的质量很重要。那就产生两个指标：真正例率(“选出的好瓜”真正的好瓜占所有的好瓜的比例，也就是好瓜的查准率)，
和假正例率(“选出的好瓜”中坏瓜占所有坏瓜的比例)。对于每一个临界概率，都有一个对应的真正例率和假正例率。
把各个临界概率对应的真正例率和假正例率绘成图就是 ROC 图。
  (6) AUC：AUC 就是 ROC 曲线中右下方区域的面积。AUC 判断一个分类用的排序队列的好坏。

11.知道了两个学习器的某个指标，比如错误率，A 学习器的错误率低于B学习器的错误率，能否认为 A 学习器质量比 B 学习器好？
   不能。首先一次的测试结果可能有误差，需要多次测试然后取平均。所以应该比较平均错误率。
   其次一个学习器的平均错误率比另一个的低，也只是一次的比较结果，可能有误差。
   若要比较两个学习器的某项置标，要用到统计学的假设检验。
   比如用 t 检验比较两个学习器的平均错误率。用方差分析和多重比较来比较多个学习器的某项性能。

12.机器学习中最简单的学习算法是什么？
   最简单的机器学习算法莫过于线性回归算法了。线性回归算法的基本形式如下：
   $$ f(x) = θ_0 * x_0 + θ_1 * x_1 + θ_2 * x_2 + ··· + θ_n * x_n $$
   其中 x 是自变量，θ 是参数。我们训练的目的就是调整 θ，使得我们输入一系列 x 值后得到的 f(x) 的解近真实值。
一个常见的例子是房价预测。通过住房面积、房间个数两个特征来预测房价，则 f(x) 就是代表房价，x0 = 1,
x1 = 住房面积，x2 = 房间个数。

13.对数几率回归模型，或者称逻辑回归(Logit Regression)就是一种基于回归的分类算法。
对数几率回归函数输出的的值是 0 到 1 范围的数。对于一个二分类任务来说，假如只分正例和反例(好瓜和坏瓜)，
那么我们可以认为函数输出的值越大，则这个样本是正例的几率越大，值越小，样本是反例的可能越大。
这就是“对数几率回归”名字的由来。我们可以定一个临界值，一般是 0.5，当输出值大于 0.5 时认为该样本是正例，否则为反例。

14.对数几率回归模型是处理二分类任务的，那处理多分类任务时怎么办？
   多分类任务可以凭借二分类任务为基础得到解决。解决思路一般有一对一，一对多和多对多三种。
   (1) 一对一策略：举个例子，现在有 A、B、C、D 四个种类，要确定某个样本属于四类中的哪一类。
那么我们可以事先训练好六个二分类的分类器 —— A/B、A/C、A/D、B/C、B/D、C/D。然后把要确定类别的样本分别放入这六个分类器。
假设分类结果分别是 A、A、A、B、D、C。可以知道六个分类器中有三个认为这个样本术语 A，认为是 B、C、D 的各有一个。
所以我们可以认为这个样本就是术语 A 类的。
   (2) 一对多策略：举个例子，现在有 A、B、C、D 四个种类，要确定某个样本属于四类中的哪一类。
那么我们可以事先训练好四个二分类的分类器 —— A/BCD、B/ACD、C/ABD、D/ABC, 分类器输出的是一个函数值。
然后把要确定类别的样本分别放入这四个分类器。假设四个分类器的结果分别是“属于 A 的概率是 0.9”，“属于 B 的概率是 0.8”、
“属于 C 的概率是 0.7”、“属于 B 的概率是 0.6”。那我们可以认为这个样本是属于 A。
   (3) 多对多策略：每次将若干个类作为正类，若干个其他类作为反类。其中需要这种策略需要特殊的设计，不能随意取。
   常用的技术：纠错输出码。工作步骤分为：
   编码：对 N 个类别做 M 次划分，每次划分将一部分类别作为正类，一部分划分为反类，从而形成一个二分类训练集；
这样一共产生 M 个训练集，可以训练出 M 个分类器。
   解码：M 个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，
返回其中距离最小的类别最为最终预测结果。
   (4) 以原书的例子做一个详细的演示：
   假如现在有一个训练数据集，可以分四个类 —— C1, C2, C3, C4.
   再具体一点可以想像为——西瓜可以分为一等瓜、二等瓜、三等瓜、四等瓜，要训练一个分类系统，使之能判断一个西瓜的等级。
   我们对训练数据集做五次划分:
     第一次，标记 C2 为正例，其他为反例，训练出一个二分类的分类器 f1;
     第二次，标记 C1、C3 为正例，其他为反例，训练出一个二分类的分类器 f2;
     第三次，标记 C3、C4 为正例，其他为反例，训练出一个二分类的分类器 f3;
     第四次，标记 C1、C2、C4 为正例，其他为反例，训练出一个二分类的分类器 f4;
     第五次，标记 C1、C4 为正例，其他为反例，训练出一个二分类的分类器 f5.
   根据这五次划分的过程，每一个类都获得了一个编码(向量)：
     C1：(-1，1，-1，1，1)
     C2：(1，-1，-1，1，-1)
     C3：(-1，1，1，-1，1)
     C4：(-1，-1，1，1，-1)
   若现在有一个测试样本，五个分类器对应的累计结果为: f1：反， f2：反， f3：正， f4：反， f5：正.
   即该测试样本对应的编码/向量为: (-1，-1，1，-1，1).
   那么分别计算这个测试样本的编码与四个类别的编码的向量距离，可以使用欧氏距离，算的与 C3 类的距离是最小的。
   因此判定该测试样本属于 C3 类。

15.决策树是什么？
   1) 决策树是模拟人类决策过程，将判断一件事情所要做的一系列决策的各种可能的集合，以数的形式展现出来的一种树形图。
   决策树是一种简单却又有用的学习算法。
   2) 决策树与普通树一样，由节点和边组成。树中每一个节点都是一个属性(特征)，或者说是对特征的判断。
根据一个节点的判断结果，决策(预测)流程走向不同的子节点，或者直接到达叶节点，即决策(预测)结束，得到结果。
   3) 典型的决策树的训练过程如下，以根据色泽、根蒂、敲声预测一个西瓜是否好瓜为例:
   (1) 输入一个数据集 D。
   (2) 生成一个节点 node。
   (3) 如果数据集中的样本全部属于同一类，比如西瓜样本全部是“好瓜”，那么 node 就是叶子节点(好瓜)。
   (4) 如果样本中的数据集不属于同一类，比如西瓜中既有好瓜也有坏瓜，那就选择一个属性，把西瓜根据选好的属性分类。
比如按照“纹理”属性，把西瓜分为清晰、模糊、稍糊三类。
   (5) 把第 4 步得到的几个数据子集作为输入数据，分别执行上面的第 1 到 5 步、直到不再执行第 4 步为止
(也就是叶子节点全部构建完成，算法结束)。

16.什么是增益率？什么是基尼系数？
   信息增益对于选项多的属性有偏好。如果我们把训练样本中每个西瓜编号，然后把编号也作为待选择属性，
那么编号肯定能带来最多的信息，最大程度降低混淆程度，所以编号这个属性肯定会被选中。但是这然并卵，
因为这样训练出来的算法对于新的样本根本不起作用。所以我们不会选择编号作为属性。类似的，如果有些属性的可选项特别多，
比如色泽现在有浅白、白、青绿、绿、墨绿五个可选项，那么色泽被选中的几率比其他属性要大。所以可以考虑用增益率代替信息增益.
   基尼系数表示从一堆样本中随机抽取两个样本，这两个样本不同类的概率，也就刚是一个是好瓜一个是坏瓜的概率。
这样，基尼系数越低，越意味着样本集中某一类比另一类样本多，也就是说，混淆程度越低。
若选择某个属性后各个划分子集的基尼系数最低，那么就选择这个属性。

17.剪枝处理的过程是怎样进行的？
   剪枝分为预剪枝和后剪枝。
   预剪枝过程就是生成决策树时，用一些新的样本去测试刚刚训练好的节点，如果这个节点的存在并不能让决策树的泛化性能提高，
也就是说分类精度或者其他衡量指标提高了，就把这个刚训练出来的节点舍弃。
   后剪枝过程是用一些新的样本来测试这棵刚刚训练出来的决策树，从下往上开始测试，
如果某个节点被换成叶节点后整一棵决策树的泛化性能提高了，也就是说分类精度或者其他衡量指标提高了, 那么就直接替换掉。

18.什么是人工神经网络？
   人工神经网络是科学及模拟人类大脑的神经神经网络建立的数学模型。人工神经网络由一个个“人工神经元”组合而成。
“人工神经元”也是一个数学模型，其本质是一个函数。所以人工神经网络的本质也是一个函数，而且是一个复杂的，包含很多变量和参数的函数。
```
