<h2>《机器学习》 :books: </h2> 

$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$

> 周志华 著  清华大学出版社 

```html
1.机器学习的本质任务是预测。
  “机器学习”也是一门学科，研究怎样使得计算机更好地学习，亦即，是一门研究“学习算法”的学科，主要任务是评估“学习算法”的好坏
以及开发新的“学习算法”。这里的“学习算法”是计算机的学习方法，本质上是一种基于现有的数据产生预测模型的算法。

2.“假设空间”里的“假设”指的是假设函数，也就是机器学习的成果。例如我们做分类学习，那么通过数据训练后得到的分类模型
就是我们得到的假设.假设空间是指所有可能假设组成的空间。也可以说是所有在表达形式上符合任务要求的假设函数的集合。
  对于西瓜分类任务，我们要获得的假设函数的形式是：好瓜 → (色泽=*) ^ (根蒂=*) ^ (敲声=*).
  假设“色泽”、“根蒂”、“敲声”3个特征都有3种可能取值，那就有 4 * 4 * 4 + 1 = 65 种可能假设，亦即假设空间的大小为 65。

3.“归纳偏好”：在西瓜分类问题中，可能由于数据集的原因，我们会得到多个符合数据集的假设函数，比如：
    好瓜 → (色泽 = 墨绿) ^ (根蒂 = 蜷缩) ^ (敲声 = 沉闷)
    好瓜 → (色泽 = 青绿) ^ (根蒂 = *) ^ (敲声 = 沉闷)
这所有训练后得到的假设组成的空间称为“版本空间”。
   那么版本空间中哪一个假设比较好？
   如果我们认为越精细越好，则选择: 好瓜 → (色泽 = 墨绿) ^ (根蒂 = 蜷缩) ^ (敲声 = 沉闷)
   如果我们认为越粗略越好，则选择: 好瓜 → (色泽 = 青绿) ^ (根蒂 = *) ^ (敲声 = 沉闷)
   像上面那样，计算机的学习算法基于某种偏好认为某个假设比其他假设好，那么我们说这个学习算法有“归纳偏好”。
事实上所有“学习算法”都有归纳偏好，而且一般来说会偏好那些形式简单的假设。

4.人类观察事物时，是通过观察事物的本质特征来认识事物的。比如观察西瓜，会观察西瓜的色泽、根蒂、敲声等特征。
  假设我们收集了一批关于西瓜的数据：
   (色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)
   (色泽 = 墨绿; 根蒂 = 稍蜷; 敲声 = 沉闷)
   (色泽 = 浅白; 根蒂 = 硬挺; 敲声 = 清脆)
    ······
   假设我们希望用这一批数据来让计算机学习:
  (1) 样本、示例、记录 —— 这批数据里的每对括号。
  (2) 数据集 —— 这组样本(示例、记录)的集合。
  (3) 特征、属性 —— 色泽、根蒂、敲声等反映一个事物的本质的可观察方面。
  (4) 属性值 —— 青绿、墨绿、蜷缩、浊响等，是属性的取值。
  (5) 属性空间、样本空间、输入空间 —— 属性长成的空间。这似乎是线性代数的语言，亦即把属性当作坐标轴，形成一个空间，
那么样本就是这个空间中一个个的点。例如：把“色泽”、“根蒂”、“敲声”作为坐标轴，则生成了一个三维空间，
每个西瓜都是这个空间里的一个点。
  (6) 维数 —— 样本空间的坐标轴数，也就是数据集的特征数量。本例中的维数是 3。
  (7) 假设 —— 也称假设函数，指计算机通过学习后得到的一个函数(预测模型)。
  (8) 标记 —— 关于样本结果的信息，比如一个(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)的西瓜是好瓜，
那么“好瓜”就是(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)这个样本的标记。
  (9) 样例 —— 带有标记的样本，比如(色泽 = 青绿; 根蒂 = 蜷缩; 敲声 = 浊响)，好瓜。
  (10) 标记空间、输出空间 —— 所有标记的集合。本例中就是指{好瓜、坏瓜}。
  (11) 泛化 —— 如果用某个数据集的样本训练出的一个模型(假设函数)，能够适用于新的样本数据，就说这个模型具有泛化能力。
模型能适用于越多的新数据，则说明其泛化能力越强。

5.NFL(No Free Lunch)定理，翻译过来就是“没有免费午餐”定理，说的是在机器学习中，在没有给定具体问题的情况下，
或者说面对的是所有问题的情况下，没有一种算法能说得上比另一种算法好。换成我们的俗话讲，就是“不存在放之四海而皆准的方法”。
只有在给定某一问题，比如说给“用特定的数据集给西瓜进行分类”，才能分析并指出某一算法比另一算法好。
这就要求我们具体问题具体分析，而不能指望找到某个算法后，就一直指望着这个“万能”的算法。
  这大概也是 no free lunch 名字的由来吧。
  定理推导的思路是证明对于某个算法 a，它在训练集以外的所有样本的误差，与 a 本身无关。
  误差是怎样表示，或者说怎样计算出来的？简单起见，只考虑二分类问题。那么误差就是分类器错判的个数与样本总数的比
      E = 误判数 / 总数。
  其次我们要明确，一个算法，会产生很多不同的假设。更详细的说，一个算法的结果就是一个函数 h，但是 h 的参数不同，
那么就会有 h1，h2 等不同的假设函数。最典型的是 h = kx + b。只要参数 k、b 不同，那么函数 h 就不同了。
  对于某个算法 a，它在训练集以外的所有样本的误差，就是它所能产生的所有假设 h，在训练集以外的所有样本上的误判率的和。
  对于某个假设 h，“h 在某个数据集上的误差”与“在某个数据集中抽取一个能让 h 误判的样本的概率”是等价的问题。
```

> NFL(No Free Lunch)定理的具体证明详见：<a href="https://www.jianshu.com/p/cbe8e0fe7b2c">url</a>

```html
6.P5 假设空间的规模问题。
  (1) 某一属性值无论取什么都合适，我们用通配符 “*” 来表示。
  (2) 世界上没有，我们用 “∅” 来表示。

7.“奥卡姆剃刀”原则：“若有多个假设与观察一致，则选最简单的那个”。
   预测((离散值：分类-二分类(正类, 反类/负类), 多分类); 连续值)。
   学习任务(监督：有标记(分类, 回归); 无监督：无标记(聚类))。
   科学推理(归纳(特殊->一般：泛化); 演绎(一般->特殊：特化))。
   归纳学习(广义(从样例中学习); 狭义(从训练数据中学得概念，最基本：布尔概念学习))。

8.评估学习器的基本思想是：
  (1) 学习器误差越小越好。对于分类任务，分类错误的样本数占总样本数的比率越小越好。
对于回归预测，预测值与真实值的差越小越好。
  (2) 学习器泛化能力越强越好。也就是说学习能力不仅在训练样本上要表现好，在新的样本上的表现也要好。
不能像书呆子一样在学校表现很好，但一进入社会就一塌糊涂。因此，我们通常衡量一个学习器的泛化误差，
也就是一个训练好的学习器在新样本上的误差表现。

9.当只有一个数据集，并且既要训练，又要测试的时候，怎么办？
  (1) 留出法：当只有一个数据集的时候，用一部分来训练，一部分来测试。而且训练数据和测试数据没有交集。
通常会用 60% 到 80% 的数据作为训练集，剩下的作为测试集。需要注意的是，在选择训练集(或者测试集)的时候要采用分层抽样的方法。
就像刷题一样，训练集和测试集都要有相近比例的题型，不能训练集全是选择题，测试集全是论述题，
应该训练集和测试集都包含选择题和测试题，而且比例要一致，都是八成选择题，两成论述题。
一次的训练-测试结果可能不够科学，最好划分不同的训练集和测试集，做多次训练-测试，将测试结果(错误率、查准率之类的)取平均。
  (2) 交叉检验法：这是在 “留出法” 的基础上改进的方法。先将数据集分为 k 个大小相似的互斥子集
(当然，每个子集的产生都要用分层抽样进行)。每次用 k-1 个子集作为训练集，剩下的一个作为测试集。
这样就可以进行 k 次训练-测试。k 的测试结果的平均值就是最终的测试结果。
  (3) 自助法：上述两种方法都是在原本作为训练集的数据中抽出一部分作为测试集，因此训练集的规模不可避免地减少了，
训练效果也就受到了影响。自助法则是一中比较好的缓解方法。假设有一个包含 m 个样本的数据集 D。
对这个数据集进行 m 次有放回的抽样，则得到了一个含有 m 个样本的数据集 D'。 D' 相对于原数据集 D，规模没有减少，
只是 D' 中有部分样本是重复出现的。所以在抽样中没有抽到的样本就作为测试集，D' 就作为训练集。按照概率论推导可知，
一般来说抽样中会有三分之一的样本没有被抽到，也就是说测试集大小为数据集 D 大小的三分之一。
   在数据集比较大时多采用留出法和交叉检验法，当数据集比较小是采用自助法。

10.在测试一个学习器时，有哪些测试指标可以使用？
  (1) 错误率(error)：最常用的测试指标就是错误率(或者精度)。对于一次分类任务，如果分类错误的样本数为 a，总样本数为 m，
则错误率 E = a / m.(精度为 1 - a / m)。比如为 100 个西瓜分类，有 10 个分错了，错误率就是 10%。
  (2) 均方误差(mean squared error)：“错误率”一般针对分类任务，回归预测则用均方误差，即各次预测值与真实值的差的平方的和：
$$ \sum_{i = 1}^n (y_预 - y_真)^2 $$; 可以认为是各次预测的误差的累加。
  (3) 查准率(precision)：也就是检索出来的结果中准确的结果所占的比例。比如找出 100 个西瓜中的好瓜，找出 50 个，
但这 50 个中只有 40 个是真正的好瓜，则查准率为 80%。
  (4) 查全率(recall)：也就是希望检索的结果中被检索出来的比例。比如找出 100 个西瓜中的好瓜，找出 40 个，
但真正的好瓜有 50个，则查全率为 80%。
  (5) ROC 曲线: 很多二分类学习器的分类方法是计算出每一个样本作为正例的概率，然后按照概率大小对样本排序，
最后确定一个临界概率(阀值)，大于临界概率的认定为正例，其余为反例。以西瓜分类为例。有些西瓜是好瓜的概率高，
有些西瓜是好瓜的概率低。把这些西瓜按照概率排序，然后取 50% 作为临界概率。概率大于 50% 的认为是好瓜，否则为坏瓜。
因此这个排序的质量很重要。那就产生两个指标：真正例率(“选出的好瓜”真正的好瓜占所有的好瓜的比例，也就是好瓜的查准率)，
和假正例率(“选出的好瓜”中坏瓜占所有坏瓜的比例)。对于每一个临界概率，都有一个对应的真正例率和假正例率。
把各个临界概率对应的真正例率和假正例率绘成图就是 ROC 图。
  (6) AUC：AUC 就是 ROC 曲线中右下方区域的面积。AUC 判断一个分类用的排序队列的好坏。

11.知道了两个学习器的某个指标，比如错误率，A 学习器的错误率低于B学习器的错误率，能否认为 A 学习器质量比 B 学习器好？
   不能。首先一次的测试结果可能有误差，需要多次测试然后取平均。所以应该比较平均错误率。
   其次一个学习器的平均错误率比另一个的低，也只是一次的比较结果，可能有误差。
   若要比较两个学习器的某项置标，要用到统计学的假设检验。
   比如用 t 检验比较两个学习器的平均错误率。用方差分析和多重比较来比较多个学习器的某项性能。

12.机器学习中最简单的学习算法是什么？
   最简单的机器学习算法莫过于线性回归算法了。线性回归算法的基本形式如下：
   $$ f(x) = θ_0 * x_0 + θ_1 * x_1 + θ_2 * x_2 + ··· + θ_n * x_n $$
   其中 x 是自变量，θ 是参数。我们训练的目的就是调整 θ，使得我们输入一系列 x 值后得到的 f(x) 的解近真实值。
一个常见的例子是房价预测。通过住房面积、房间个数两个特征来预测房价，则 f(x) 就是代表房价，x0 = 1,
x1 = 住房面积，x2 = 房间个数。

13.对数几率回归模型，或者称逻辑回归(Logit Regression)就是一种基于回归的分类算法。
对数几率回归函数输出的的值是 0 到 1 范围的数。对于一个二分类任务来说，假如只分正例和反例(好瓜和坏瓜)，
那么我们可以认为函数输出的值越大，则这个样本是正例的几率越大，值越小，样本是反例的可能越大。
这就是“对数几率回归”名字的由来。我们可以定一个临界值，一般是 0.5，当输出值大于 0.5 时认为该样本是正例，否则为反例。

14.对数几率回归模型是处理二分类任务的，那处理多分类任务时怎么办？
   多分类任务可以凭借二分类任务为基础得到解决。解决思路一般有一对一，一对多和多对多三种。
   (1) 一对一策略：举个例子，现在有 A、B、C、D 四个种类，要确定某个样本属于四类中的哪一类。
那么我们可以事先训练好六个二分类的分类器 —— A/B、A/C、A/D、B/C、B/D、C/D。然后把要确定类别的样本分别放入这六个分类器。
假设分类结果分别是 A、A、A、B、D、C。可以知道六个分类器中有三个认为这个样本术语 A，认为是 B、C、D 的各有一个。
所以我们可以认为这个样本就是术语 A 类的。
   (2) 一对多策略：举个例子，现在有 A、B、C、D 四个种类，要确定某个样本属于四类中的哪一类。
那么我们可以事先训练好四个二分类的分类器 —— A/BCD、B/ACD、C/ABD、D/ABC, 分类器输出的是一个函数值。
然后把要确定类别的样本分别放入这四个分类器。假设四个分类器的结果分别是“属于 A 的概率是 0.9”，“属于 B 的概率是 0.8”、
“属于 C 的概率是 0.7”、“属于 B 的概率是 0.6”。那我们可以认为这个样本是属于 A。
   (3) 多对多策略：每次将若干个类作为正类，若干个其他类作为反类。其中需要这种策略需要特殊的设计，不能随意取。
   常用的技术：纠错输出码。工作步骤分为：
   编码：对 N 个类别做 M 次划分，每次划分将一部分类别作为正类，一部分划分为反类，从而形成一个二分类训练集；
这样一共产生 M 个训练集，可以训练出 M 个分类器。
   解码：M 个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，
返回其中距离最小的类别最为最终预测结果。
   (4) 以原书的例子做一个详细的演示：
   假如现在有一个训练数据集，可以分四个类 —— C1, C2, C3, C4.
   再具体一点可以想像为——西瓜可以分为一等瓜、二等瓜、三等瓜、四等瓜，要训练一个分类系统，使之能判断一个西瓜的等级。
   我们对训练数据集做五次划分:
     第一次，标记 C2 为正例，其他为反例，训练出一个二分类的分类器 f1;
     第二次，标记 C1、C3 为正例，其他为反例，训练出一个二分类的分类器 f2;
     第三次，标记 C3、C4 为正例，其他为反例，训练出一个二分类的分类器 f3;
     第四次，标记 C1、C2、C4 为正例，其他为反例，训练出一个二分类的分类器 f4;
     第五次，标记 C1、C4 为正例，其他为反例，训练出一个二分类的分类器 f5.
   根据这五次划分的过程，每一个类都获得了一个编码(向量)：
     C1：(-1，1，-1，1，1)
     C2：(1，-1，-1，1，-1)
     C3：(-1，1，1，-1，1)
     C4：(-1，-1，1，1，-1)
   若现在有一个测试样本，五个分类器对应的累计结果为: f1：反， f2：反， f3：正， f4：反， f5：正.
   即该测试样本对应的编码/向量为: (-1，-1，1，-1，1).
   那么分别计算这个测试样本的编码与四个类别的编码的向量距离，可以使用欧氏距离，算的与 C3 类的距离是最小的。
   因此判定该测试样本属于 C3 类。

15.决策树是什么？
   1) 决策树是模拟人类决策过程，将判断一件事情所要做的一系列决策的各种可能的集合，以数的形式展现出来的一种树形图。
   决策树是一种简单却又有用的学习算法。
   2) 决策树与普通树一样，由节点和边组成。树中每一个节点都是一个属性(特征)，或者说是对特征的判断。
根据一个节点的判断结果，决策(预测)流程走向不同的子节点，或者直接到达叶节点，即决策(预测)结束，得到结果。
   3) 典型的决策树的训练过程如下，以根据色泽、根蒂、敲声预测一个西瓜是否好瓜为例:
   (1) 输入一个数据集 D。
   (2) 生成一个节点 node。
   (3) 如果数据集中的样本全部属于同一类，比如西瓜样本全部是“好瓜”，那么 node 就是叶子节点(好瓜)。
   (4) 如果样本中的数据集不属于同一类，比如西瓜中既有好瓜也有坏瓜，那就选择一个属性，把西瓜根据选好的属性分类。
比如按照“纹理”属性，把西瓜分为清晰、模糊、稍糊三类。
   (5) 把第 4 步得到的几个数据子集作为输入数据，分别执行上面的第 1 到 5 步、直到不再执行第 4 步为止
(也就是叶子节点全部构建完成，算法结束)。

16.什么是增益率？什么是基尼系数？
   信息增益对于选项多的属性有偏好。如果我们把训练样本中每个西瓜编号，然后把编号也作为待选择属性，
那么编号肯定能带来最多的信息，最大程度降低混淆程度，所以编号这个属性肯定会被选中。但是这然并卵，
因为这样训练出来的算法对于新的样本根本不起作用。所以我们不会选择编号作为属性。类似的，如果有些属性的可选项特别多，
比如色泽现在有浅白、白、青绿、绿、墨绿五个可选项，那么色泽被选中的几率比其他属性要大。所以可以考虑用增益率代替信息增益.
   基尼系数表示从一堆样本中随机抽取两个样本，这两个样本不同类的概率，也就刚是一个是好瓜一个是坏瓜的概率。
这样，基尼系数越低，越意味着样本集中某一类比另一类样本多，也就是说，混淆程度越低。
若选择某个属性后各个划分子集的基尼系数最低，那么就选择这个属性。

17.剪枝处理的过程是怎样进行的？
   剪枝分为预剪枝和后剪枝。
   预剪枝过程就是生成决策树时，用一些新的样本去测试刚刚训练好的节点，如果这个节点的存在并不能让决策树的泛化性能提高，
也就是说分类精度或者其他衡量指标提高了，就把这个刚训练出来的节点舍弃。
   后剪枝过程是用一些新的样本来测试这棵刚刚训练出来的决策树，从下往上开始测试，
如果某个节点被换成叶节点后整一棵决策树的泛化性能提高了，也就是说分类精度或者其他衡量指标提高了, 那么就直接替换掉。

18.什么是人工神经网络？
   人工神经网络是科学及模拟人类大脑的神经神经网络建立的数学模型。人工神经网络由一个个“人工神经元”组合而成。
“人工神经元”也是一个数学模型，其本质是一个函数。所以人工神经网络的本质也是一个函数，而且是一个复杂的，
包含很多变量和参数的函数。

19.什么是人工神经元？
   人工神经元是人工神经网络的基本单元，其本质也是一个函数。类似人类大脑的神经元，
人工神经元模型也有树突、轴突、神经元中心等结构。最经典的神经元模型是“M-P神经元模型”。
一个神经元接收来自其他神经元传来的信号(变量)，通过树突传输(参数)到神经元中心(参数)，经过转换后(映射 f)，
通过突触传递出去(函数值)。由上述神经元工作过程可知，一个神经元的可以接受多个变量，输出一个结果。
一个神经元的输出也可能是另一个神经元的输入。所以一个由多个神经元组成的神经网络，本质上就是一个复杂的、多层嵌套的复合函数。

20.神经元有哪些分类？
   按照神经元的层数来分，可以分为只有输入、输出层的单层网络(不计算输入层)，代表是感知机；以及包含隐含层的多层网络。
   按照网络结构来分，可以分为径向基函数网络(RBF)、竞争学习网络(ART)、自组织映射网络(SOM)、级联相关网络、
递归神经网络(RNN)、Boltzman 机等各种各样纷繁复杂的种类。

21.感知机有什么作用？怎么训练一个感知机？
   感知机是最简单的神经网络，只有输入和输出层。一般来说，感知机用来进行二分类任务，或者实现逻辑“与”、“或”、“非”的操作。
   感知机模型 f(x) = sign(w ⋅ x + b) 的输出一般是二值的，即 1 或 -1。当自变量小于 0 时输出 -1，否则输出 1。
感知机的学习规则很简单，也是类似与线性回归一样，使用梯度下降的思想，每读取一个样本，计算一次预测值，就调整一次参数(权值)。

22.多层网络有什么作用，其训练过程是怎样的？
   多层神经网络由于神经元层数比单层多，所以参数更多，也意味着学习能力更强，所以能够胜任图像处理、语音识别等任务。
   最经典的多层网络算法是“误差逆传播算法(BP)”。其基本思想也是通过构建关于参数的损失函数来衡量神经网络的误差，
然后通过梯度下降等方法令损失函数取最小值，求得参数的取值。
   多层神经网络的参数也是按层划分的，所以参数的求解也要一层一层地进行。首先求出输出层的各个参数。
然后通过输出层的参数求解出倒数第二层的参数，再通过倒数第二层的参数求解出倒数第三层的参数，以此类推，求解出所有层的参数。
   由于参数是从输出层往后求解的，并且是通过损失函数计算出来的，因此成为误差逆传播算法。因其推导过程无比繁琐，在此不做赘述。
```

> 误差逆传播算法(BP)的推导过程详见：<a href="http://www.cnblogs.com/wentingtu/archive/2012/06/05/2536425.html">url</a>

```html
23.支持向量机是什么？
   支持向量机不是一台“机”，而是一个算法。一般来说认为这是一个分类算法，而且分类效果极佳。
   以基本的二分类任务为例。给定一堆训练数据，假设每个样本只有两个属性。我们试图在坐标系中找出一条线，把不同类的样本分开来。
从图中可以看出，有多条直线可以把两类样本分开。但是从直觉上就可以察觉，在两类点中间的那条线(加粗那条)，是最合适的。
因为这条线离两类样本的距离都适中，因此最能适应新样本，也就是说泛化能力最强。
   图中有很多样本，但真正决定这条线的位置和方向的，只有两个点，就是每个类的点中离直线最近的点。
因为它们代表了各自所在类的边界，只有它们的变化会引起线的位置变化，所以说这两个点支持着这条线。这些样本在坐标系中表现为点，
用代数表示就是向量(每个分量是对应的属性值)。这就是“支持向量机”算法名称的由来。

24.怎样找出支持向量机用来分类的那根线？
   支持向量机在样本属性只有两个的时候任务是找一条线，在属性有三个的时候就是找一个平面。
在样本属性有三个以上的时候找的是“超平面”。线、平面，都是超平面的特例，所以他们都可以用一个方程来表示：ω^T ⋅ x + b = 0,
这个方程就是我们要找的超平面的代数表示。其参数 ω 和 b 就是我们要确定的参数，也就是我们要学习的目标。
这个超平面也可以认为是使得分类函数 f(x) = 0 的点集： f(x) = ω^T ⋅ x + b。
   支持向量机的分类方式略有别于 logit 回归 —— logit 回归是把样本值代入分类函数后得出一个 0 到 1 之间的数，
根据这个数是否大于某个阀值(如 0.5)来进行分类。支持向量机则是把样本值代入分类函数后得出一个实数，
根据这个数是否大于 0 来分类。而且在用训练数据训练过程中，会设置约束条件，
让所有样本数据的计算值在 “> 1” 和 “< -1” 之间。也就是在两个类别之间留出一道间隙。
有空间几何理论可以知道，空间中一个点到超平面的距离是: r = |ω^T ⋅ x + b| / ||ω||.
   因为把样本值代入分类函数后得出的计算值在 “> 1” 和 “< -1” 之间，所以超平面能正确分类的前提(即约束条件)之一是:
   ω^T ⋅ xi + b >= +1, yi = +1; ω^T ⋅ xi + b <= -1, yi = -1.
   那么离超平面最近的两个样本，代入分类函数后的计算值必然分别是 1 和 -1。所以两个类之间的间隙就是这两个点到超平面的距离的和，
也就是：γ = 2 / ||ω||.
   要使得支持向量机效果最好，也就是说最能适应新样本，也就意味着使得两类训练样本之间的间隙最大化，所以我们训练的具体目标就是
—— 求得参数 ω 和 b 使得两类样本间的间隙最大，同时还要让各个样本代入分类函数后的计算值大于 1(或小于 -1)。
   min(ω, b) (1 / 2) ⋅ ||ω|| ^ 2.
   s.t yi(ω^T ⋅ xi + b) >= 1, i = 1, 2, ..., m.
   这个最值问题的求解过程相当烧脑，用到对偶问题思想、拉格朗日乘子法、KKT 条件、SMO 算法等，略过。
当我们把训练数据一个个输入支持向量机，逐渐调整参数 ω 和 b，最后得到了分类函数。这就是支持向量机的基本形式。

25.什么是贝叶斯分类器？
   以贝叶斯方法为核心原理构造出来的分类程序统称为贝叶斯分类器，主要包括朴素贝叶斯分类器，半朴素贝叶斯分类器，以及贝叶斯网络。

26.贝叶斯方法是什么？
   贝叶斯方法是一位英国数学家托马斯·贝叶斯在 18 世纪提出来的概率学方法。
   贝叶斯学派似乎已经占据了概率学派中的半壁江山(另外一半江山归于频率学派)。
   举个例子：已知: 100 名男生中有 99 个短发，1 个长发；100 名女生中有 90 个长发，10 个短发。
   从这 200 名学生中抽取一个学生，已知是男生，则该男生是长发的概率是多少？答案很简单，是 0.01000。
   P(长发|男生) = P(长发，男生) / P(男生)。
   从中 200 名学生中抽取一个学生，已知是长发，则该长发学生是男生的概率是多少？答案稍难一点，是 0.01099。
   P(男生|长发) = P(长发|男生) * P(男生) / P(长发)。
   由此可见，大多数的概率计算方法(古典概率模型、几何概率模型、条件概率公式、全概率公式、联合概率公式···)都是已知前提条件
发生的情况下求各种可能结果发生的概率。而贝叶斯公式则是用于已知一件事情已经发生，求导致其发生的各种可能前提条件成立的概率。

27.怎样应用贝叶斯方法构造分类器？
   我们继续以西瓜分类问题为例，用下面的色泽、根蒂等 8 个特征的数据，判断一个西瓜是好瓜还是坏瓜。
   应用贝叶斯方法的基本思想是，分别求出待分类西瓜是好瓜以及坏瓜的概率，哪一个概率高，就将其归到那一类。
     P(好瓜=是|色泽=青绿，根蒂=蜷缩，···)
     P(好瓜=否|色泽=青绿，根蒂=蜷缩，···)
   这两个概率怎么求？
   根据贝叶斯方法，基本公式是如下: P(c|x) = P(c) * P(x|c) / P(x) = P(c) / P(x) * ∏(d, i=1)P(xi|c);
   其中 c 是类别，向量 x 是特征值。其中分子 P(x) 对于 P(c=好瓜|x) 和 P(c=坏瓜|x) 是一样的，可以一并舍去。
   所以关键要求出 P(c) * P(x|c)，也就是 P(c) * ∏(d, i=1)P(xi|c).
   亦即: P(好瓜=是) * P(色泽=？|好瓜) * P(根蒂=？|好瓜) ··· *
        P(好瓜=否) * P(色泽=？|坏瓜) * P(根蒂=？|坏瓜) ··· *
   这两条式子中基本上每一项都是一个简单的条件概率(甚至可以用古典概型求出)。
   数据集中的密度和含糖率是连续数据，它们的条件概率怎么求？可以假设这些连续数据服从正态分布，
然后算出训练数据集中这两项属性的均值和方差，代入公式即可算出。
   用训练数据集算出各个特征的条件概率以及各个类别出现的概率以后，就可以应用贝叶斯公式预测一个西瓜是好瓜以及坏瓜的概率。
   在上面的例子中，(1) 用到了贝叶斯公式; (2) 假设各个属性互不影响(实际上不是，比如敲声可能受密度影响).
   所以我们刚刚构造出的分类器叫做朴素贝叶斯分类器 (Naive Bayesian classifier)。
```
